[
    {
        "title": "How I used Polars to build built functime, a next gen ML forecasting library",
        "abstract": "Everybody knows Polars revolutionised the dataframe landscape, yet fewer realise that machine learning is next. Thanks to its extreme speed, we can speed up feature engineering by 1-2 orders of magnitude. The true gains, however, span across the whole ML lifecycle, with significantly faster batch inference and effortless scaling (no PySpark required!). Add a best-of-the-class set of tools for feature extraction, model evaluation and diagnostic visualisations and you'll get functime: a next-generation library for ML forecasting. Though time-series practitioners are the primary audience, there's something for all data scientists. It's not just forecasting: it's about building the next generation of machine learning libraries.",
        "description": null,
        "date": "2023-12-06 16:00-16:30 UTC",
        "place": "Virtual",
        "site": "https://pydata.org/global2023/",
        "page": "https://pydata.org/global2023/schedule/html_export/html/talk/VVLLEW/",
        "youtube": null
    },
    {
        "title": "I know Polars is fast, but my data pipelines are written in pandas!",
        "abstract": "We all know it by now: Polars is blazingly fast. Yet my pipelines are all written in pandas, and it will just take too much time to rewrite them in Polars... won't it? Turns out, it takes less than thirty minutes to tame this new arctic beast!",
        "description": null,
        "date": "2023-09-08 14:05",
        "place": "Coimbra Business School, Coimbra, Portugal",
        "site": "https://2023.pycon.pt/",
        "page": "https://pretalx.evolutio.pt/pycon-pt-2023/speaker/W9ZLUD/",
        "youtube": null
    },
    {
        "title": "Rust is easy",
        "abstract": null,
        "description": "An intro talk that starts from Python-like code inside a Rust program and iteratively fixes compiler errors to reach valid Rust, showing how Rust catches mistakes early and why its constraints can improve code quality.",
        "date": "2023-05-27",
        "place": "Florence, Italy",
        "site": "https://2023.pycon.it/",
        "page": "https://orobix.com/en/pycon-it-2023-wow/",
        "youtube": null
    },
    {
        "title": "Is the great data frame showdown finally over? Enter: polars",
        "abstract": null,
        "description": "A deep dive into Polars at PyCon Italia 2023, discussing lazy and eager APIs and where Polars beats or differs from pandas for real-world data workloads.",
        "date": null,
        "place": "Florence, Italy",
        "site": "https://2023.pycon.it/",
        "page": "https://datasciencemilan.medium.com/a-journey-into-pycon-italia-2023-6e4b1e849c45",
        "youtube": null
    },
    {
        "title": "Build your first Python package with Rust",
        "abstract": "Are you tired of that colleague of yours that boasts about how great of a package manager cargo is, and how Python packaging ecosystem sucks in comparison? Or do you want to know how Python libraries with a Rust core like ruff and polars are built? You've come to the right place: we will guide you trough how to use PyO3 and maturin to package and publish your first, blazingly fast, rust-powered Python package!",
        "description": null,
        "date": "2023-03-09 19:00-21:00 GMT+1",
        "place": "LocalHost Crafted Software, Via Adelaide Bono Cairoli, 20127 Milano MI, Italy",
        "site": "https://community.codemotion.com/",
        "page": "https://community.codemotion.com/python-milano/meetups/a-guide-to-asyncio--python-packages-with-rust",
        "youtube": null
    },
    {
        "title": "functime: a next generation ML forecasting library powered by Polars",
        "abstract": "Polars is mature, production ready, intuitive to write and pleasant to read. And it is fast. Thanks to Rust and Rayon, you can achieve speeds greater than numba's. If you combine it with top-of-the-class evaluation methods, not only can you get speedups of about 1-2x order of magnitude in feature engineering and cross-validation, but also dramatically improve your development workflow. That's what we set out to demonstrate with functime. We chose to write a time-series library first, because forecasting can be a costly undertaking, with significant problems of scale. Making predictions with big panel datasets usually required fitting thousands of univariate models, one at a time, using distributed systems. On the other hand, functime unlocks an efficient forecasting workflow, from your laptop. This talk is a hands-on demonstration for forecasting practitioners and data scientists alike. It will showcase how to build clean and performant forecasting pipelines with rich feature-engineering capabilities, enabling a seamless and more efficient modelling workflow. Nevertheless, the principles behind functime can be grasped by every machine learning practitioner: forecasting is just a use-case to show off Polars' potential. With Polars, we can improve the current state of machine learning modelling and raise the ceiling for what reasonable scale means.",
        "description": "Powered by this impressive query engine, functime enables forecasting thousands of time series at once, from the comfort of your laptop. With Polars, we can push the boundary for what reasonable scale means and build a new generation of tools for machine learning.",
        "date": "2024-05-23 11:00-11:30",
        "place": "Grand Hotel Mediterraneo, Florence, Italy",
        "site": "https://2024.pycon.it/",
        "page": "https://2024.pycon.it/en/event/functime-a-next-generation-ml-forecasting-library-powered-by-polars",
        "youtube": null
    },
    {
        "title": "Embeddings, Transformers, RLHF: Three key ideas to understand ChatGPT",
        "abstract": "ChatGPT has become a groundbreaking tool, transforming how professionals in various industries work. However, while many articles focus on \"the 30 prompts everybody needs to know\", they often overlook the underlying technology of ChatGPT. To truly understand ChatGPT, it's important to comprehend three key concepts: Embeddings (how LLMs convert words and phrases into numerical values), Transformers (deep-learning modules that capture semantic connections), and RLHF (Reinforcement Learning with Human Feedback) to align models with intended purposes and ethical standards. The talk also covers the four primary steps involved in building and training a GPT-like model, plus limitations and strengths of current generative AI models and actionable insights for safe adoption.",
        "description": "Everyone is using ChatGPT, but few know how it works. This talk walks through the main ideas powering ChatGPT to lift the curtain on the system behind the magic.",
        "date": "2024-05-25 14:45-15:15",
        "place": "Grand Hotel Mediterraneo, Florence, Italy",
        "site": "https://2024.pycon.it/",
        "page": "https://2024.pycon.it/en/event/embeddings-transformers-rlhf-three-key-ideas-to-understand-chatgpt",
        "youtube": "https://www.youtube.com/watch?v=91Bbn0TYSoM"
    },
    {
        "title": "Uncertainty estimation at scale with functime, Polars and conformal predictions",
        "abstract": "functime is a modern time-series forecasting library to generate predictions for thousands of time series at once, while never leaving your laptop. Thanks to Polars' powerful query engine, feature extraction and cross-validation are 1-2 orders of magnitude faster. Plus, functime offers a best-of-the-class set of diagnostic tools to further streamline your workflow. In this talk, we'll learn how to use functime to analyse your model and generate blazingly fast prediction intervals using EnBPI, a state-of-the-art conformal prediction framework that is also available in other popular Python packages.",
        "description": null,
        "date": "2024-06-15 16:30-17:10 Europe/London",
        "place": "Leonardo Royal Hotel London Tower Bridge, 45 Prescot St, London E1 8GP, United Kingdom",
        "site": "https://pydata.org/london2024/",
        "page": "https://pydata.org/london2024/schedule/html_export/html/talk/AKS33W/",
        "youtube": null
    },
    {
        "title": "Foundational Models for Time Series Forecasting: are we there yet?",
        "abstract": "Transformers are everywhere: NLP, Computer Vision, sound generation and even protein-folding. Why not in forecasting? After all, what ChatGPT does is predicting the next word. Why this architecture isn't state-of-the-art in the time series domain? In this talk, you will understand how Amazon Chronos and Salesforece's Moirai transformer-based forecasting models work, the datasets used to train them and how to evaluate them to see if they are a good fit for your use-case. We begin by exploring the use-cases where transformer-based forecasters excel in the time series domain and introduce the concept of foundational models for forecasting (TSFM). We then discuss data availability, synthetic data, and analyze state-of-the-art TSFMs, with a focus on Chronos (AWS) and TimesFM (Google), before outlining future research directions and benchmarking guidance.",
        "description": null,
        "date": "2024-12-04 18:00-18:30 UTC",
        "place": "Virtual",
        "site": "https://pydata.org/global2024",
        "page": "https://global2024.pydata.org/cfp/talk/UA8DFF/",
        "youtube": null
    },
    {
        "title": "Bigger models or more data? The new scaling laws for LLMs",
        "abstract": "The incredibly famous Chinchilla paper changed the way we train LLMs. The authors, including the current Mistral CEO, outlined the scaling laws to maximise your model performance under a compute budget, balancing the number of parameters and training tokens. Today, these heuristics are in jeopardy. LLaMA-3, for one, is trained on an unreasonable amount of tokens of text, but this is why it's so good. How much data do we actually need to train LLMs? This talk will shed light on the latest trends in model training and perhaps suggest newer scaling laws.",
        "description": null,
        "date": "2024-06-17 11:30-12:15",
        "place": "Talent Garden Calabiana, Milano, Italy",
        "site": "https://www.aiconf.it/2024",
        "page": "https://www.improove.tech/videos/3480/-Bigger-models-or-more-data-The-new-scaling-laws-for-LLMs",
        "youtube": null
    },
    {
        "title": "Foundational Models for Time Series Forecasting: are we there yet?",
        "abstract": "This is a pragmatic talk, tailored for Generative AI and ML practitioners. While the talk explains how transformer-based architectures are adapted to forecasting problems, the true goal is to get a lay of the land of the publicly available datasets, the data that was used to train the model, and see through the benchmarks that have been published. The first part explains how researchers discretised time series into a finite dictionary and the limitations of this approach, such as handling different sampling frequencies. We then make a parallel with LLM scaling laws to evaluate the data strategies used to train universal forecasting models, and show how these models compare against robust baselines. No specific prior knowledge is required.",
        "description": "Transformers have changed fields from Computer Vision to protein-folding. But what about forecasting? Come learn how titans like Amazon and Google train their foundational models, and how not to fall for the hype and learn how to read public benchmarks.",
        "date": "2025-05-30 15:35-16:05",
        "place": "Savoia Hotel Regency, Bologna, Italy",
        "site": "https://2025.pycon.it/",
        "page": "https://2025.pycon.it/en/event/foundational-models-for-time-series-forecasting-are-we-there-yet",
        "youtube": null
    },
    {
        "title": "OpenAI o1/o3: the New Scaling Laws for LLMs that can reason",
        "abstract": "With o1, OpenAI ushered a new era of LLMs: reasoning capabilities. This new breed of models broadened the concept of scaling laws, shifting focus from train-time to test-time (or inference-time) compute. How do these models work? What do we think their architectures look like, and what data do we use to train them? And finally, and perhaps more importantly: how expensive can they get, and what can we use them for?",
        "description": null,
        "date": "2025-06-25 15:30-16:15",
        "place": "Talent Garden Calabiana, Milano, Italy",
        "site": "https://www.aiconf.it/",
        "page": "https://www.aiconf.it/e/sessione/3704/OpenAI-o1-o3-the-New-Scaling-Laws-for-LLMs-that-can-reason",
        "youtube": null
    },
    {
        "title": "LLM Inference Arithmetics: the Theory behind Model Serving",
        "abstract": "Have you ever asked yourself how parameters for an LLM are counted, or wondered why Gemma 2B is actually closer to a 3B model? You have no clue about what a KV-Cache is? (And, before you ask: no, it's not a Redis fork.) Do you want to find out how much GPU VRAM you need to run your model smoothly? If your answer to any of these questions was \"yes\", or you have another doubt about inference with LLMs - such as batching, or time-to-first-token - this talk is for you. Well, except for the Redis part.",
        "description": "The talk covers the math behind transformer inference in an accessible way, including parameter counts, memory vs. compute, prefill vs. decoding, and how batching affects inference metrics like time-to-first-token.",
        "date": "2025-04-23 14:30-15:00",
        "place": "Darmstadt, Germany",
        "site": "https://2025.pycon.de/",
        "page": "https://2025.pycon.de/talks/G3AT7E/",
        "youtube": null
    },
    {
        "title": "LLM Inference Arithmetics: the Theory behind Model Serving",
        "abstract": "Have you ever asked yourself how parameters for an LLM are counted, or wondered why Gemma 2B is actually closer to a 3B model? You have no clue about what a KV-Cache is? (And, before you ask: no, it's not a Redis fork.) Do you want to find out how much GPU VRAM you need to run your model smoothly? If your answer to any of these questions was \"yes\", or you have another doubt about inference with LLMs - such as batching, or time-to-first-token - this talk is for you. Well, except for the Redis part.",
        "description": "The talk covers the math behind transformer inference in an accessible way, including parameter counts, memory vs. compute, prefill vs. decoding, and how batching affects inference metrics like time-to-first-token.",
        "date": "2025-06-07 16:15-17:00",
        "place": "Convene Sancroft, St. Paul's (Sancroft, Rose St, Paternoster Sq., London EC4M 7DQ, United Kingdom)",
        "site": "https://pydata.org/london2025",
        "page": "https://cfp.pydata.org/london2025/talk/DDJWLB/",
        "youtube": null
    }
]
